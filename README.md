# Chatbot Service - UCB Commerce

Microservicio de asistencia inteligente basado en **RAG (Retrieval-Augmented Generation)** para la plataforma UCB Commerce. Este servicio actÃºa como el nÃºcleo cognitivo de la atenciÃ³n al cliente, proporcionando respuestas contextualizadas en tiempo real sobre productos, inventario y procesos institucionales.

## ðŸš€ CaracterÃ­sticas Principales

- **Arquitectura RAG Avanzada**: Combina la potencia de modelos LLM (Llama 3.1 vÃ­a Groq) con una base de conocimiento vectorial dinÃ¡mica.
- **SincronizaciÃ³n en Tiempo Real**: Mantiene una consistencia estricta con el catÃ¡logo de productos. Cualquier cambio en precios, stock o disponibilidad en el `products-service` se refleja instantÃ¡neamente en los embeddings del chatbot, garantizando que el asistente nunca ofrezca informaciÃ³n obsoleta.
- **DiseÃ±o Modular**: Estructurado siguiendo los principios de *Clean Architecture* para facilitar la escalabilidad y el mantenimiento.
- **Base de Conocimiento HÃ­brida**: Integra informaciÃ³n institucional estÃ¡tica (polÃ­ticas, horarios) con datos transaccionales dinÃ¡micos (productos).

## ðŸ›  Stack TecnolÃ³gico

- **Runtime**: Python 3.10+
- **Framework Web**: FastAPI (High performance async framework)
- **Vector Database**: Supabase (pgvector)
- **LLM Inference**: Groq (Llama 3.1-8b-instant)
- **Embeddings**: OpenAI (text-embedding-3-small)

## ðŸ“‚ Arquitectura del Proyecto

El proyecto ha sido refactorizado para seguir una estructura modular:

```
app/
â”œâ”€â”€ core/
â”‚   â””â”€â”€ config.py          # GestiÃ³n de configuraciÃ³n y clientes (Singleton pattern)
â”œâ”€â”€ routers/
â”‚   â””â”€â”€ chat.py            # DefiniciÃ³n de endpoints API (Interface Layer)
â”œâ”€â”€ services/
â”‚   â””â”€â”€ rag_service.py     # LÃ³gica de negocio RAG (Chunking, Embedding, Retrieval)
â””â”€â”€ main.py                # Punto de entrada y composiciÃ³n de la aplicaciÃ³n
```

## ðŸ”„ Flujo de Datos y Consistencia

1.  **Ingesta de Datos**:
    - Documentos institucionales se cargan vÃ­a endpoint `/upload`.
    - **Productos**: Se sincronizan automÃ¡ticamente desde el `products-service` mediante hooks de eventos.
2.  **GeneraciÃ³n de Embeddings**: Se utiliza `text-embedding-3-small` para vectorizar la informaciÃ³n.
3.  **Almacenamiento**: Los vectores se persisten en la tabla `rag_ucbcommerce_chunks` de Supabase.
4.  **RecuperaciÃ³n (Retrieval)**:
    - Ante una consulta del usuario, se genera el embedding de la pregunta.
    - Se realiza una bÃºsqueda de similitud semÃ¡ntica (cosine similarity) en Supabase.
5.  **GeneraciÃ³n (Generation)**:
    - El contexto recuperado se inyecta en el prompt del sistema.
    - El LLM genera una respuesta precisa basada estrictamente en la evidencia proporcionada.

## ðŸš€ InstalaciÃ³n y Despliegue

### Prerrequisitos

- Python 3.10+
- Acceso a Supabase (con extensiÃ³n `vector` habilitada)
- API Keys de OpenAI y Groq

### Pasos

1.  **Instalar dependencias:**
    ```bash
    pip install -r requirements.txt
    ```

2.  **ConfiguraciÃ³n de Entorno (`.env`):**
    ```env
    OPENAI_API_KEY=sk-...
    GROQ_API_KEY=gsk_...
    SUPABASE_URL=https://...
    SUPABASE_SERVICE_ROLE_KEY=...
    ```

3.  **Ejecutar el servicio:**
    ```bash
    uvicorn app.main:app --reload --port 8004
    ```

## ðŸ“¡ Endpoints Principales

- `POST /chat`: Endpoint principal de interacciÃ³n. Acepta `question` y devuelve `answer` + `chunks_used`.
- `POST /upload`: Carga de documentos de texto plano para conocimiento institucional estÃ¡tico.